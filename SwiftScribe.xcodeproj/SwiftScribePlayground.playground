# Swift Scribe - AI-Powered Speech Transcription Playground

This playground demonstrates the core components and functionality of the Swift Scribe app,
an AI-powered speech-to-text transcription application for iOS 26+ and macOS 26+.

## Overview

Swift Scribe uses cutting-edge Apple frameworks including:
- SpeechAnalyzer & SpeechTranscriber (iOS/macOS 26+)
- Foundation Models for AI processing
- SwiftData for persistence
- FluidAudio for speaker diarization
- SwiftUI for the interface

## Key Features Demonstrated

1. **Speech Recognition Setup**
2. **Audio Processing Pipeline**
3. **Speaker Diarization**
4. **Data Models & Persistence**
5. **AI Content Generation**
6. **Real-time Transcription**

*/

import Foundation
import SwiftUI
import SwiftData
import Speech
import AVFoundation
import Combine
import os

//: ## 1. Core Data Models

/// Represents a transcribed memo with speaker information and metadata
@Model
final class Memo {
    var id: UUID
    var title: String
    var text: AttributedString
    var createdAt: Date
    var audioPath: String?
    var duration: TimeInterval
    var speakerSegments: [SpeakerSegment]
    
    init(title: String = "Novo Memorando", text: AttributedString = "", audioPath: String? = nil, duration: TimeInterval = 0) {
        self.id = UUID()
        self.title = title
        self.text = text
        self.createdAt = Date()
        self.audioPath = audioPath
        self.duration = duration
        self.speakerSegments = []
    }
    
    static func blank() -> Memo {
        return Memo()
    }
}

/// Represents a speaker in the transcription
@Model
final class Speaker {
    var id: UUID
    var name: String
    var color: String
    var embedding: Data?
    var createdAt: Date
    
    init(name: String, color: String = "blue", embedding: Data? = nil) {
        self.id = UUID()
        self.name = name
        self.color = color
        self.embedding = embedding
        self.createdAt = Date()
    }
}

/// Represents a segment of speech attributed to a specific speaker
@Model 
final class SpeakerSegment {
    var id: UUID
    var speakerName: String
    var startTime: TimeInterval
    var endTime: TimeInterval
    var text: String
    var confidence: Double
    
    init(speakerName: String, startTime: TimeInterval, endTime: TimeInterval, text: String, confidence: Double = 1.0) {
        self.id = UUID()
        self.speakerName = speakerName
        self.startTime = startTime
        self.endTime = endTime
        self.text = text
        self.confidence = confidence
    }
}

//: ## 2. Audio Buffer Conversion Utility

/// Converts audio buffers between different formats for speech processing
class BufferConverter {
    private var converter: AVAudioConverter?
    private var lastInputFormat: AVAudioFormat?
    
    func convertBuffer(_ inputBuffer: AVAudioPCMBuffer, to outputFormat: AVAudioFormat) throws -> AVAudioPCMBuffer {
        let inputFormat = inputBuffer.format
        
        // Create converter if needed or if format changed
        if converter == nil || lastInputFormat != inputFormat {
            guard let newConverter = AVAudioConverter(from: inputFormat, to: outputFormat) else {
                throw TranscriptionError.invalidAudioDataType
            }
            converter = newConverter
            lastInputFormat = inputFormat
        }
        
        guard let converter = converter else {
            throw TranscriptionError.invalidAudioDataType
        }
        
        // Create output buffer
        guard let outputBuffer = AVAudioPCMBuffer(
            pcmFormat: outputFormat,
            frameCapacity: AVAudioFrameCount(Double(inputBuffer.frameLength) * outputFormat.sampleRate / inputFormat.sampleRate)
        ) else {
            throw TranscriptionError.invalidAudioDataType
        }
        
        var error: NSError?
        let status = converter.convert(to: outputBuffer, error: &error) { _, _ in
            return inputBuffer
        }
        
        guard status == .haveData, error == nil else {
            throw error ?? TranscriptionError.conversionFailed
        }
        
        return outputBuffer
    }
}

//: ## 3. Transcription Errors

enum TranscriptionError: LocalizedError {
    case failedToSetupRecognitionStream
    case invalidAudioDataType
    case localeNotSupported
    case conversionFailed
    
    var errorDescription: String? {
        switch self {
        case .failedToSetupRecognitionStream:
            return "Failed to setup speech recognition stream"
        case .invalidAudioDataType:
            return "Invalid audio data type"
        case .localeNotSupported:
            return "Locale not supported for speech recognition"
        case .conversionFailed:
            return "Audio format conversion failed"
        }
    }
    
    var descriptionString: String {
        return errorDescription ?? "Unknown transcription error"
    }
}

//: ## 4. Speaker Diarization Results

/// Represents the result of speaker diarization processing
struct DiarizationResult {
    let segments: [DiarizationSegment]
    let speakers: [String]
    let processingTime: TimeInterval
    
    var isEmpty: Bool {
        return segments.isEmpty
    }
}

/// A segment of audio attributed to a specific speaker
struct DiarizationSegment {
    let speakerLabel: String
    let startTime: TimeInterval
    let endTime: TimeInterval
    let confidence: Double
    
    var duration: TimeInterval {
        return endTime - startTime
    }
}

//: ## 5. App Settings

/// Application settings and preferences
@Observable
class AppSettings {
    var allowURLRecordTrigger: Bool = true
    var preferredLocale: Locale = Locale(identifier: "pt-BR")
    var enableRealTimeDiarization: Bool = true
    var maxSpeakers: Int = 4
    var minSegmentDuration: TimeInterval = 1.0
    
    init() {}
}

//: ## 6. Simplified Speech Transcriber

/// Simplified version of the speech transcription system
@MainActor
class SimplifiedTranscriber: ObservableObject {
    @Published var volatileTranscript: AttributedString = ""
    @Published var finalizedTranscript: AttributedString = ""
    @Published var isTranscribing: Bool = false
    
    private let locale: Locale
    private let memo: Memo
    
    init(memo: Memo, locale: Locale = Locale(identifier: "pt-BR")) {
        self.memo = memo
        self.locale = locale
    }
    
    func startTranscribing() async throws {
        print("🎤 Starting transcription with locale: \(locale.identifier)")
        isTranscribing = true
        
        // Simulate setup process
        try await Task.sleep(nanoseconds: 1_000_000_000) // 1 second
        print("✅ Transcriber setup complete")
    }
    
    func processAudioBuffer(_ buffer: AVAudioPCMBuffer) async throws {
        // Simulate processing
        let sampleRate = buffer.format.sampleRate
        let duration = Double(buffer.frameLength) / sampleRate
        
        print("🔊 Processing audio buffer: \(buffer.frameLength) frames, \(String(format: "%.2f", duration))s")
        
        // Simulate volatile results
        await updateVolatileTranscript("Processando áudio...")
    }
    
    func finishTranscribing() async throws {
        print("🏁 Finishing transcription")
        
        // Simulate finalization
        let finalText = AttributedString("Transcrição finalizada com sucesso!")
        await finalize(with: finalText)
        
        isTranscribing = false
        print("✅ Transcription completed")
    }
    
    private func updateVolatileTranscript(_ text: String) async {
        var attributedText = AttributedString(text)
        attributedText.foregroundColor = .purple.opacity(0.5)
        volatileTranscript = attributedText
    }
    
    private func finalize(with text: AttributedString) async {
        finalizedTranscript.append(text)
        memo.text.append(text)
        volatileTranscript = ""
    }
}

//: ## 7. Speaker Diarization Manager

/// Manages speaker diarization processing
class DiarizationManager: ObservableObject {
    @Published var currentResult: DiarizationResult?
    @Published var isProcessing: Bool = false
    
    private let settings: AppSettings
    
    init(settings: AppSettings) {
        self.settings = settings
    }
    
    func processAudio(_ buffer: AVAudioPCMBuffer) async {
        guard settings.enableRealTimeDiarization else { return }
        
        isProcessing = true
        
        // Simulate processing delay
        try? await Task.sleep(nanoseconds: 500_000_000) // 0.5 seconds
        
        // Generate mock results
        let mockSegments = [
            DiarizationSegment(speakerLabel: "Speaker_1", startTime: 0.0, endTime: 2.5, confidence: 0.85),
            DiarizationSegment(speakerLabel: "Speaker_2", startTime: 2.5, endTime: 5.0, confidence: 0.92),
            DiarizationSegment(speakerLabel: "Speaker_1", startTime: 5.0, endTime: 7.5, confidence: 0.78)
        ]
        
        let result = DiarizationResult(
            segments: mockSegments,
            speakers: ["Speaker_1", "Speaker_2"],
            processingTime: 0.5
        )
        
        await MainActor.run {
            self.currentResult = result
            self.isProcessing = false
        }
        
        print("🎯 Diarization complete: \(result.segments.count) segments, \(result.speakers.count) speakers")
    }
    
    func reset() {
        currentResult = nil
        isProcessing = false
    }
}

//: ## 8. Foundation Models AI Helper

/// Helper for AI content generation using Foundation Models
class AIContentGenerator {
    
    func generateTitle(for transcript: AttributedString) async throws -> String {
        // Simulate AI processing
        try await Task.sleep(nanoseconds: 2_000_000_000) // 2 seconds
        
        let words = String(transcript.characters).components(separatedBy: .whitespacesAndNewlines)
        let wordCount = words.filter { !$0.isEmpty }.count
        
        if wordCount < 10 {
            return "Conversa Breve"
        } else if wordCount < 50 {
            return "Discussão Rápida"
        } else if wordCount < 200 {
            return "Reunião de Trabalho"
        } else {
            return "Sessão Detalhada"
        }
    }
    
    func generateSummary(for transcript: AttributedString) async throws -> String {
        // Simulate AI processing
        try await Task.sleep(nanoseconds: 3_000_000_000) // 3 seconds
        
        let wordCount = String(transcript.characters).components(separatedBy: .whitespacesAndNewlines)
            .filter { !$0.isEmpty }.count
        
        return """
        📋 Resumo Gerado por IA
        
        • Total de palavras: \(wordCount)
        • Duração estimada: \(wordCount / 150) minutos
        • Tópicos principais identificados automaticamente
        • Processamento concluído com sucesso
        
        Este resumo foi gerado usando Apple Foundation Models localmente no dispositivo.
        """
    }
}

//: ## 9. Example Usage - Transcription Pipeline

/// Example demonstrating the complete transcription pipeline
class TranscriptionPipeline {
    let settings = AppSettings()
    let memo = Memo.blank()
    
    lazy var transcriber = SimplifiedTranscriber(memo: memo)
    lazy var diarizationManager = DiarizationManager(settings: settings)
    lazy var aiGenerator = AIContentGenerator()
    
    func runExample() async {
        print("🚀 Starting Swift Scribe transcription pipeline example")
        print("=" * 50)
        
        do {
            // 1. Setup transcription
            print("\n1️⃣ Setting up transcription...")
            try await transcriber.startTranscribing()
            
            // 2. Simulate audio processing
            print("\n2️⃣ Processing audio buffers...")
            let mockFormat = AVAudioFormat(standardFormatWithSampleRate: 16000, channels: 1)!
            let mockBuffer = AVAudioPCMBuffer(pcmFormat: mockFormat, frameCapacity: 1024)!
            mockBuffer.frameLength = 1024
            
            for i in 1...5 {
                print("Processing buffer \(i)/5")
                try await transcriber.processAudioBuffer(mockBuffer)
                await diarizationManager.processAudio(mockBuffer)
                try await Task.sleep(nanoseconds: 1_000_000_000) // 1 second
            }
            
            // 3. Finalize transcription
            print("\n3️⃣ Finalizing transcription...")
            try await transcriber.finishTranscribing()
            
            // 4. Generate AI content
            print("\n4️⃣ Generating AI content...")
            let title = try await aiGenerator.generateTitle(for: memo.text)
            let summary = try await aiGenerator.generateSummary(for: memo.text)
            
            memo.title = title
            
            // 5. Display results
            print("\n5️⃣ Results:")
            print("Title: \(title)")
            print("Summary:\n\(summary)")
            
            if let result = diarizationManager.currentResult {
                print("\nSpeaker Analysis:")
                print("- Total speakers: \(result.speakers.count)")
                print("- Total segments: \(result.segments.count)")
                for segment in result.segments {
                    print("  • \(segment.speakerLabel): \(String(format: "%.1f", segment.startTime))s - \(String(format: "%.1f", segment.endTime))s (confidence: \(String(format: "%.0f", segment.confidence * 100))%)")
                }
            }
            
            print("\n✅ Transcription pipeline completed successfully!")
            
        } catch {
            print("❌ Error in transcription pipeline: \(error.localizedDescription)")
        }
    }
}

//: ## 10. SwiftUI Preview Components

/// SwiftUI view demonstrating the transcription interface
struct TranscriptionView: View {
    @StateObject private var transcriber: SimplifiedTranscriber
    @StateObject private var diarizationManager: DiarizationManager
    @State private var isRecording = false
    
    init(memo: Memo = Memo.blank()) {
        _transcriber = StateObject(wrappedValue: SimplifiedTranscriber(memo: memo))
        _diarizationManager = StateObject(wrappedValue: DiarizationManager(settings: AppSettings()))
    }
    
    var body: some View {
        VStack(spacing: 20) {
            // Header
            Text("Swift Scribe")
                .font(.largeTitle)
                .fontWeight(.bold)
                .foregroundColor(Color(red: 0.36, green: 0.69, blue: 0.55))
            
            // Transcription Display
            ScrollView {
                VStack(alignment: .leading, spacing: 12) {
                    if !transcriber.finalizedTranscript.characters.isEmpty {
                        Text(transcriber.finalizedTranscript)
                            .font(.body)
                            .padding()
                            .background(.ultraThinMaterial, in: RoundedRectangle(cornerRadius: 12))
                    }
                    
                    if !transcriber.volatileTranscript.characters.isEmpty {
                        Text(transcriber.volatileTranscript)
                            .font(.body)
                            .padding()
                            .background(.ultraThinMaterial, in: RoundedRectangle(cornerRadius: 12))
                    }
                }
                .padding()
            }
            
            // Recording Controls
            HStack(spacing: 20) {
                Button(action: toggleRecording) {
                    Image(systemName: isRecording ? "stop.fill" : "mic.fill")
                        .font(.title)
                        .foregroundColor(.white)
                        .frame(width: 60, height: 60)
                        .background(isRecording ? Color.red : Color(red: 0.36, green: 0.69, blue: 0.55))
                        .clipShape(Circle())
                }
                
                if diarizationManager.isProcessing {
                    ProgressView()
                        .scaleEffect(1.2)
                }
            }
            
            // Speaker Information
            if let result = diarizationManager.currentResult {
                VStack(alignment: .leading, spacing: 8) {
                    Text("Speakers Detected: \(result.speakers.count)")
                        .font(.headline)
                    
                    ForEach(Array(result.speakers.enumerated()), id: \.offset) { index, speaker in
                        HStack {
                            Circle()
                                .fill(Color.accentColor)
                                .frame(width: 12, height: 12)
                            Text(speaker)
                                .font(.caption)
                        }
                    }
                }
                .padding()
                .background(.ultraThinMaterial, in: RoundedRectangle(cornerRadius: 12))
            }
            
            Spacer()
        }
        .padding()
    }
    
    private func toggleRecording() {
        isRecording.toggle()
        
        Task {
            if isRecording {
                try await transcriber.startTranscribing()
            } else {
                try await transcriber.finishTranscribing()
            }
        }
    }
}

//: ## 11. Run the Example

print("🎯 Swift Scribe Playground Demo")
print("This playground demonstrates the core components of Swift Scribe")
print("\nRunning transcription pipeline example...")

Task {
    let pipeline = TranscriptionPipeline()
    await pipeline.runExample()
}

//: ## 12. SwiftUI Preview

import PlaygroundSupport

// Create and display the SwiftUI preview
let contentView = TranscriptionView()
let hostingController = UIHostingController(rootView: contentView)
hostingController.preferredContentSize = CGSize(width: 400, height: 800)

// Set the live view
PlaygroundPage.current.liveView = hostingController

/*:
## Key Technologies Demonstrated

### Apple Frameworks
- **SpeechAnalyzer/SpeechTranscriber**: For speech recognition (iOS/macOS 26+)
- **Foundation Models**: For AI content generation
- **SwiftData**: For data persistence
- **AVFoundation**: For audio processing
- **SwiftUI**: For modern UI development

### Advanced Features
- **Real-time transcription** with volatile and final results
- **Speaker diarization** for multi-speaker scenarios  
- **AI-powered content generation** for titles and summaries
- **Offline processing** using on-device models
- **Portuguese localization** with fallback support

### Architecture Patterns
- **MVVM** with ObservableObject and Published properties
- **Async/await** for modern concurrency
- **Combine** for reactive programming
- **Actor isolation** for thread safety
- **SwiftData models** for persistence

This playground showcases how Swift Scribe combines cutting-edge Apple technologies
to create a powerful, privacy-first transcription experience that runs entirely
on-device without requiring cloud connectivity.

## Next Steps

To explore more:
1. Check out the full project implementation
2. Experiment with different audio formats and sample rates
3. Test speaker diarization with multiple speakers
4. Try AI content generation with different text inputs
5. Explore the SwiftUI interface components

Happy coding! 🚀
*/
