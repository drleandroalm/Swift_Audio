//
//  LLMRequest.swift
//  Aurora
//
//  Created by Dan Murrell Jr on 8/19/24.
//

import AuroraCore
import Foundation

/// A struct representing the input to be sent to the Language Learning Model (LLM).
/// This structure allows customization of various parameters to influence the response generated by the LLM.
public struct LLMRequest {
    /// An array of `LLMMessage` instances representing either a single prompt or a conversation history.
    public let messages: [LLMMessage]

    /// The sampling temperature to control randomness (values between 0.0 to 1.0). Higher values (e.g., 1.0) make the output more random, while lower values (e.g., 0.0) make it more deterministic.
    public let temperature: Double

    /// The maximum number of tokens in the generated response.
    public let maxTokens: Int

    /// The specific LLM model to use for processing (e.g., "gpt-3.5-turbo", "davinci"). If not specified, the default model for the service will be used.
    public let model: String?

    /// Whether or not to stream the response (default is `false`).
    public let stream: Bool

    /// Optional system prompt that overrides the service's default system prompt
    public let systemPrompt: String?

    /// Additional less commonly used configuration options to influence the behavior of the LLM.
    public let options: LLMRequestOptions?

    /// Initializes a new `LLMRequest` with customizable parameters and reasonable defaults for most fields.
    ///
    /// - Parameters:
    ///    -  messages: An array of `LLMMessage` objects representing the input, either as a single message or a conversation history.
    ///    -  temperature: A value between 0.0 and 1.0 controlling the randomness of the response (default is 0.7).
    ///    -  maxTokens: The maximum number of tokens to generate in the response (default is 256).
    ///    -  model: An optional string representing the model to use (default is nil, meaning the default model for the service will be used).
    ///    -  stream: Whether or not the response should be streamed (default is `false`).
    ///    -  systemPrompt: An optional string representing a custom system prompt to use and override the service system prompt (default is nil).
    ///    -  options: An optional `LLMRequestOptions` object containing less commonly used parameters to customize the response generation.
    public init(
        messages: [LLMMessage],
        temperature: Double = 0.7,
        maxTokens: Int = 256,
        model: String? = nil,
        stream: Bool = false,
        systemPrompt: String? = nil,
        options: LLMRequestOptions? = nil
    ) {
        self.messages = messages
        self.temperature = temperature
        self.maxTokens = maxTokens
        self.model = model
        self.stream = stream
        self.systemPrompt = systemPrompt
        self.options = options
    }

    // Method to estimate the token count of the request messages
    public func estimatedTokenCount() -> Int {
        return messages.reduce(0) { $0 + $1.content.estimatedTokenCount() }
    }
}
