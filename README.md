# Swift Scribe - AI-Powered Speech-to-Text Private Transcription App for iOS 26 & macOS 26+
[![Swift](https://img.shields.io/badge/Swift-6.1+-orange.svg)](https://swift.org) ![Build Target](https://img.shields.io/badge/macOS-ARM64-success)

> **Real-time voice transcription, advanced speaker diarization, on-device AI processing, and intelligent note-taking exclusively for iOS 26 & macOS 26 and above**

Uses Apple's new Foundation Model Framework and SpeechTranscriber. Requires macOS 26 to run and compile the project. The goal is to demonstrate how easy it is now to build local, AI-first apps.

The goal of this is mostly to act as an example for others looking to work with the new models and [FluidAudio](https://github.com/FluidInference/FluidAudio). We will probably not actively maintain this unless there's significant traction. If you have problem, please consider joining our discord to chat more about this! 

[![Discord](https://img.shields.io/badge/Discord-Join%20Chat-7289da.svg)](https://discord.gg/WNsvaCtmDe)

## ğŸ¯ Overview

**Swift Scribe** is a privacy-first, AI-enhanced transcription application built exclusively for iOS 26/macOS 26+ that transforms spoken words into organized, searchable notes with professional-grade speaker identification. Using Apple's latest SpeechAnalyzer and SpeechTranscriber frameworks (available only in iOS 26/macOS 26+) combined with FluidAudio's advanced speaker diarization and on-device Foundation Models, it delivers real-time speech recognition, intelligent speaker attribution, content analysis, and advanced text editing capabilities.

Live transcription updates are now driven by an `ObservableObject` transcriber with `@Published` properties, observed via `@StateObject` in SwiftUI. This yields ultraâ€‘smooth volatile text rendering while recording, with safe mainâ€‘thread updates.


### ğŸ“¦ FluidAudio Integration (No Packages, Offlineâ€‘Only)

- FluidAudio is vendored directly under `Scribe/Audio/FluidAudio/` (diarizer + minimal shared utilities). There is no Swift Package dependency.
- The app target operates strictly offline for diarization. Model resolution order:
  1) `FLUID_AUDIO_MODELS_PATH` (if set)
  2) App bundle resource `speaker-diarization-coreml/`
  3) Repo folder `speaker-diarization-coreml/`
- ASR and VAD sources remain inâ€‘repo but are compiled into a separate static library target `FluidAudio-ASR` (not linked to the app). This preserves code without impacting the app module or its concurrency guarantees.
- See `Flui_Audio_Integration.md` for a stepâ€‘byâ€‘step log of the integration.

### ğŸ™ï¸ Microphone Selection + 16 kHz Pipeline

- Smart Microphone Selector (default): mimics the operating systemâ€™s current microphone selection. On macOS, the app prefers the builtâ€‘in microphone when possible.
- Manual Override (Settings â†’ Microphone): toggle â€œSelecionar microfone manualmenteâ€ to pick an input from the available sources. When enabled, this bypasses the smart selector.
- Latencyâ€‘optimized conversion to 16 kHz:
  - The input tap is installed at the deviceâ€™s native format (e.g., 48 kHz on Bluetooth) for stability; a reusable AVAudioConverter converts each buffer to 16 kHz mono Float32 for ML consumers and onâ€‘disk storage.
  - Bluetooth inputs use a slightly larger tap buffer (4096) to reduce HAL overloads; other inputs use 2048.


![Swift Scribe Demo - AI Speech-to-Text Transcription](Docs/swift-scribe.gif)

![Swift Scribe Demo - AI Speech-to-Text Transcription iOS](Docs/phone-scribe.gif)

## ğŸ›  Technical Requirements & Specifications

### **System Requirements**
- **iOS 26 Beta or newer** (REQUIRED - will not work on iOS 25 or earlier)
- **macOS 26 Beta or newer** (REQUIRED - will not work on macOS 25 or earlier)  
- **Xcode Beta** with latest Swift 6.2+ toolchain
- **Swift 6.2+** programming language
- **Apple Developer Account** with beta access to iOS 26/macOS 26
- **Microphone permissions** for speech input


## ğŸš€ Installation & Setup Guide

### **Development Installation**

1. **Clone the repository:**

   ```bash
   git clone https://github.com/seamlesscompute/swift-scribe
   cd swift-scribe
   ```

2. **Open in Xcode Beta:**

   ```bash
   open SwiftScribe.xcodeproj
   ```

3. **Configure deployment targets** for iOS 26 Beta/macOS 26 Beta or newer

4. **Build and run** using Xcode Beta with Swift 6.2+ toolchain

### Persistent Storage (SwiftData)
- Data is now persisted to disk using SwiftData with a container that includes `Memo`, `Speaker`, and `SpeakerSegment`. Your recordings, speakers and analytics survive app restarts. No additional setup is required.

### Exact Speaker Attribution
- The final speaker attribution uses precise token-time alignment: each wordâ€™s `audioTimeRange` is mapped to the diarization segment covering its timestamp. This produces exact color spans and accurate â€œFalantesâ€ text por segmento.
- A mesma coloraÃ§Ã£o precisa pode ser aplicada ao texto principal em tempo real (toggle em ConfiguraÃ§Ãµes > ExibiÃ§Ãµes e Analytics).

### Post-Recording Analytics
- The â€œFalantesâ€ view includes an analytics panel (toggle): perâ€‘speaker total time, number of turns, and percentage of the session with a mini bar chart.
- Feature toggles are available under ConfiguraÃ§Ãµes > ExibiÃ§Ãµes e Analytics.

### ğŸ™ï¸ InscriÃ§Ã£o, verificaÃ§Ã£o, aprimoramento e renomeaÃ§Ã£o de falantes
- A partir da visÃ£o â€œFalantesâ€, clique em â€œInscrever falanteâ€ para abrir uma folha de inscriÃ§Ã£o.
- Digite o nome do falante, pressione â€œGravarâ€ e fale por ~8 segundos (barra de progresso exibida), depois â€œPararâ€ e â€œSalvarâ€. TambÃ©m Ã© possÃ­vel:
  - Acumular vÃ¡rias amostras (capturando mÃºltiplos clipes) antes de salvar; as embeddings sÃ£o fundidas automaticamente (mÃ©dia) para um perfil mais robusto.
  - Importar um arquivo de Ã¡udio local (macOS: WAV/M4A/MP3/CAF/AIFF) para usar como amostra.
- O perfil do falante (nome + embedding) Ã© persistido no SwiftData, e Ã© injetado no runtime do diarizador para reconhecimento imediato e consistente.
- Para verificar se um trecho de Ã¡udio corresponde a um falante salvo, use â€œVerificar semelhanÃ§aâ€ no menu do chip do falante: grave ou importe um trecho curto e veja a confianÃ§a (0â€“100%).
- Para renomear um falante existente, use o botÃ£o de lÃ¡pis no chip do falante (ou o menu de contexto â†’ Renomear).
- Para aprimorar um falante existente com novas amostras, use â€œAprimorarâ€ no menu do chip. Grave/importe um ou mais clipes, aplique, e a embedding serÃ¡ fundida com a anterior para maior robustez.

### ğŸ” Importar/Exportar perfis de falantes (macOS)
- â€œExportarâ€ (no cabeÃ§alho da visÃ£o Falantes) gera um JSON com os perfis (`id`, `nome`, `cor`, `embedding`).
- â€œImportarâ€ carrega perfis do JSON e injeta no runtime para uso imediato.

âš ï¸ **Note**: Ensure your device is running iOS 26+ or macOS 26+ before installation.

### Offline Model Assets (No Network Required)

Swift Scribe bundles the full diarization model suite inside the app for fully offline startup:

- `speaker-diarization-coreml/` is packaged into the app bundle and includes:
  - `pyannote_segmentation.mlmodelc` (speech activity segmentation)
  - `wespeaker_v2.mlmodelc` (speaker embeddings)
  - Additional variants (`wespeaker.mlmodelc`, `wespeaker_int8.mlmodelc`) and `.mlpackage` manifests

At runtime, the app resolves models in this order:

1) `FLUID_AUDIO_MODELS_PATH` environment override
2) App bundle resource `speaker-diarization-coreml/`
3) Repository checkout `speaker-diarization-coreml/` (development only)

No download is attempted when the bundle assets are present. This guarantees diarization works entirely offline.

## ğŸ“‹ Use Cases & Applications

**Transform your workflow with AI-powered transcription:**

### **Business & Professional**
- ğŸ“Š **Meeting transcription** with automatic speaker identification and minute generation
- ğŸ“ **Interview recording** with real-time speaker diarization and attribution
- ğŸ’¼ **Business documentation** with speaker-tagged content and report creation
- ğŸ¯ **Sales call analysis** with participant tracking and follow-up automation

### **Healthcare & Medical**
- ğŸ¥ **Medical dictation** and clinical documentation
- ğŸ‘¨â€âš•ï¸ **Patient interview transcription** with medical terminology
- ğŸ“‹ **Healthcare report generation** and chart notes
- ğŸ”¬ **Research interview analysis** and coding

### **Education & Academic**
- ğŸ“ **Lecture transcription** with chapter segmentation
- ğŸ“š **Study note creation** from audio recordings
- ğŸ” **Research interview analysis** with theme identification
- ğŸ“– **Language learning** with pronunciation feedback

### **Legal & Compliance**
- âš–ï¸ **Court proceeding transcription** with timestamp accuracy
- ğŸ“‘ **Deposition recording** and legal documentation
- ğŸ›ï¸ **Legal research** and case note compilation
- ğŸ“‹ **Compliance documentation** and audit trails

### **Content Creation & Media**
- ğŸ™ï¸ **Podcast transcription** with automatic speaker labeling and show note generation
- ğŸ¬ **Video content scripting** with professional speaker diarization
- âœï¸ **Article writing** from multi-speaker voice recordings
- ğŸ“º **Content creation workflows** with speaker-attributed production notes

### **Accessibility & Inclusion**
- ğŸ¦» **Real-time captions** for hearing-impaired users
- ğŸ—£ï¸ **Speech accessibility tools** with customizable formatting
- ğŸŒ **Multi-language accessibility** support
- ğŸ¯ **Assistive technology integration**

## ğŸ— Project Architecture & Code Structure

```
Scribe/                     # Core application logic and modules
â”œâ”€â”€ Audio/                  # Audio capture, processing, and FluidAudio speaker diarization
â”œâ”€â”€ Transcription/         # SpeechAnalyzer and SpeechTranscriber implementation
â”œâ”€â”€ AI/                    # Foundation Models integration and AI processing
â”œâ”€â”€ Views/                 # SwiftUI interface with rich text editing
â”œâ”€â”€ Models/                # Data models for memos, transcription, speakers, and AI
â”œâ”€â”€ Storage/               # Local data persistence and model management
â””â”€â”€ Extensions/            # Swift extensions and utilities
```

**Key Components:**

- **Audio Engine** - Real-time audio capture and preprocessing
- **Speech Pipeline** - SpeechAnalyzer integration and transcription flow
- **Speaker Diarization** - FluidAudio integration for professional speaker identification
- **AI Processing** - Foundation Models for content analysis
- **Rich Text System** - AttributedString with speaker attribution and advanced formatting
- **Data Layer** - SwiftData integration with speaker models and local storage
- **Localization & Settings** - Interface em portuguÃªs brasileiro com painel de ajustes para diarizaÃ§Ã£o e processamento em tempo real

## â­ Advanced Features

### **ğŸ¤ Professional Speaker Diarization**
- **FluidAudio Integration**: Industry-grade speaker identification and clustering
- **Research-Grade Performance**: Competitive with academic benchmarks (17.7% DER on AMI dataset)
- **Real-time Processing**: Live speaker identification during recording with minimal latency
- **Speaker Attribution**: Color-coded transcription with confidence scores and timeline mapping

### **ğŸ§  Intelligent Speaker Management**
- **Automatic Speaker Detection**: No manual configuration required
- **Speaker Persistence**: Consistent speaker identification across recording sessions  
- **Visual Attribution**: Rich text formatting with speaker-specific colors and metadata
- **Speaker Analytics**: Detailed insights into speaking patterns and participation

### **ğŸ”’ Privacy-First Architecture**
- **Fully On-Device**: All processing happens locally - no cloud dependencies
- **Zero Data Transmission**: Audio and speaker data never leave your device
- **Secure Storage**: Speaker embeddings and models stored securely with SwiftData
- **Complete Offline Operation**: Works without internet connectivity

### **ğŸ‡§ğŸ‡· LocalizaÃ§Ã£o e ConfiguraÃ§Ãµes Inteligentes**
- Interface completa em portuguÃªs brasileiro, incluindo fluxos de gravaÃ§Ã£o, visualizaÃ§Ã£o de falantes e preferÃªncias.
- Painel de configuraÃ§Ãµes com controles de diarizaÃ§Ã£o (limite de clusterizaÃ§Ã£o, duraÃ§Ã£o mÃ­nima, nÃºmero mÃ¡ximo de falantes), alternÃ¢ncia de processamento em tempo real e tamanho da janela de processamento em tempo real (1â€“10 s).
- Presets de perfil de diarizaÃ§Ã£o: "ReuniÃ£o" (mÃºltiplos falantes, janelas curtas), "Entrevista" (dois falantes, segmentos mais longos) e "Podcast" (2â€“4 falantes com equilÃ­brio de estabilidade e troca). Ajustes manuais passam o perfil para "Personalizado".
- BotÃ£o "Restaurar padrÃ£o" reaplica rapidamente os valores do perfil selecionado (desativado no modo Personalizado).
- Dicas interativas: explica o impacto do limite de agrupamento e da duraÃ§Ã£o mÃ­nima com exemplos prÃ¡ticos e simples.
- PreferÃªncias persistidas via SwiftData + `UserDefaults`, aplicadas automaticamente ao `DiarizationManager` durante e apÃ³s a captura.
- CompatÃ­vel com modelos locais em `speaker-diarization-coreml/` ou em caminhos personalizados via `FLUID_AUDIO_MODELS_PATH`.
- Prompts de IA sÃ£o gerados pelo `DefaultMemoAIContentGenerator` com instruÃ§Ãµes em portuguÃªs brasileiro para tÃ­tulos e resumos.

## âœ… Testes Automatizados
- `ScribeTests/ScribeTests.swift`: garante que `AppSettings` carregue valores padrÃ£o corretos e reflita alteraÃ§Ãµes de execuÃ§Ã£o.
- `ScribeTests/DiarizationManagerTests.swift`: cobre processamento em tempo real, finalizaÃ§Ã£o manual, validaÃ§Ã£o de Ã¡udio e guarda auxiliares ao injetar diarizadores simulados.
- `ScribeTests/SwiftDataPersistenceTests.swift`: valida a persistÃªncia de `Speaker`/`SpeakerSegment` em um contÃªiner SwiftData em memÃ³ria, e o reaproveitamento de perfis existentes.
- `ScribeTests/MemoAIFlowTests.swift`: exercita o fluxo de IA para tÃ­tulos e resumos com geradores simulados, incluindo fallbacks localizados.
- `ScribeTests/OfflineModelsVerificationTests.swift`: verifica a presenÃ§a dos diretÃ³rios de modelos (`pyannote_segmentation.mlmodelc`, `wespeaker_v2.mlmodelc`) dentro do bundle do app, garantindo startup offline.
- `ScribeTests/RecordingFlowTests.swift`: simula sessÃµes extensas, diarizaÃ§Ã£o desativada e reconfiguraÃ§Ã£o dinÃ¢mica de preferÃªncias usando transcritor e diarizador falsos.

### 2025-09 Fixes â€” Robustez de GravaÃ§Ã£o
- O ciclo de captura agora roda em `Task.detached` sem checar cancelamentos, evitando abortos espÃºrios causados por ruÃ­do do HAL.
- `stopRecording()` aguarda o tÃ©rmino do task (`await value`) antes de gerar sumÃ¡rio/IA, garantindo teardown limpo.
- Foi adicionado `Recorder.hasReceivedAudio` para sinalizar a chegada do primeiro buffer e auxiliar diagnÃ³sticos.
- Watchdog de 3s reinicia o engine automaticamente se nenhum buffer chegar (reinstala tap e motor sem encerrar a captura), lidando com falhas transitÃ³rias do HAL.
- No macOS desativamos explicitamente o modo de voice processing do input (`setVoiceProcessingEnabled(false)`) para evitar erros `AUVoiceProcessing` (-10877) que travavam a captura em hardwares sem suporte.
- Indicador visual de â€œmic prontoâ€ (primeiro buffer) no cabeÃ§alho.
- Picker de dispositivo de entrada (macOS) na tela de ConfiguraÃ§Ãµes (padrÃ£o: embutido).

### Headless / Smoke
- `Scripts/RecorderSmokeTest/run_smoke_test.sh`: executa `TranscriberSmokeTests` (XCTest), com prÃ©-conversÃ£o para o formato do analyzer.
- `Scripts/RecorderSmokeCLI/run_cli.sh`: roda uma CLI Swift que imprime `[CLI][volatile]` e `[CLI][final]` para um WAV conhecido.
- ObservaÃ§Ã£o: em macOS hÃ¡ flutuaÃ§Ãµes do XCTest ao finalizar (`nilError`) com o pipeline da Apple. No CI e em execuÃ§Ãµes determinÃ­sticas, prefira a CLI (`RecorderSmokeCLI`). Os testes de smoke no macOS sÃ£o marcados para `XCTSkip` por padrÃ£o.

### Captura de Logs 80s (OSLog)
- `Scripts/capture_80s_markers.sh`: compila, inicia o binÃ¡rio do app diretamente (`Contents/MacOS/SwiftScribe`), coleta 80s de OSLog e imprime marcadores.
- VariÃ¡veis de ambiente suportadas:
  - `SS_AUTO_RECORD=1` (apenas Debug) â€” inicia a gravaÃ§Ã£o automaticamente ao abrir.
  - `FLUID_AUDIO_MODELS_PATH=/caminho/para/speaker-diarization-coreml` â€” evita aviso de warmup quando a versÃ£o Debug procura no bundle.
- Marcadores em inglÃªs e ptâ€‘BR sÃ£o analisados: â€œAVAudioEngine startedâ€, â€œRecording session startingâ€, â€œNo audio detectedâ€, â€œRecorder did stop with causeâ€, â€œDiarization manager initializedâ€, â€œMotor de gravaÃ§Ã£o iniciadoâ€, â€œPrimeiro buffer recebidoâ€, â€œNenhum Ã¡udio detectadoâ€, `dispositivo=`.

### Autoâ€‘record e modo headless (Debug)
- Defina `SS_AUTO_RECORD=1` para que o app crie um novo memorando, selecione a tela de gravaÃ§Ã£o e inicie automaticamente ao abrir.
- Alternativamente, passe o argumento `--headless-record` na linha de comando do app para o mesmo efeito.
- Em macOS, o gravador passa a â€œvincularâ€ o dispositivo de entrada atual e reassertÃ¡â€‘lo em reinÃ­cios do watchdog, reduzindo oscilaÃ§Ã£o do HAL.

### Xcode 26.1 â€” Alinhamento de Build Settings
- Projeto:
  - Debug: `SWIFT_COMPILATION_MODE=singlefile`, `SWIFT_STRICT_CONCURRENCY=complete`
  - Release: `SWIFT_COMPILATION_MODE=wholemodule`, `SWIFT_OPTIMIZATION_LEVEL=-O`, `SWIFT_STRICT_CONCURRENCY=complete`
  - `CLANG_ANALYZER_LOCALIZABILITY_NONLOCALIZED=YES`
- Alvo App:
  - Release: `STRIP_INSTALLED_PRODUCT=YES`
  - (Demais recomendaÃ§Ãµes jÃ¡ habilitadas)
- A janela de proteÃ§Ã£o para toque de parada Ã© de 2s apÃ³s o inÃ­cio; depois disso, parar Ã© permitido mesmo sem buffers recebidos (duplo toque confirma).
- Em iOS, o comutador de modos no `TranscriptView` foi simplificado com `AnyView` para reduzir a complexidade do typeâ€‘checker.

Execute `xcodebuild -scheme SwiftScribe -destination 'platform=macOS' test` apÃ³s alteraÃ§Ãµes para garantir que todas as suÃ­tes passem. A execuÃ§Ã£o local atual cobre 18 testes em ~1,6 s em um Mac arm64.

### ğŸš€ CI/CD Pipeline e Performance Tracking

âœ… **Status: CI Totalmente Funcional com macOS 26 Runner**

Os workflows do GitHub Actions estÃ£o **ativos e funcionando** usando o runner `macos-26-arm64`:
- **Runner**: macOS 26 ARM64
- **Xcode DisponÃ­vel**: 26.0 (build 17A324) + 16.4
- **Compatibilidade**: âœ… Projeto requer iOS 26.0/macOS 26.0 (deployment targets)

**Workflows executam automaticamente** em cada push para `main` ou PR.

**Testes Locais**: TambÃ©m suportados com Xcode 26.0+ (veja comandos abaixo).

---

**GitHub Actions Workflow** (`.github/workflows/ci-test-suite.yml`):
- **6 jobs paralelos** executados em cada push para `main` ou PR
  1. macOS Build + Testes de Unidade (ARM64)
  2. iOS Build + Testes no Simulador (iPhone 16 Pro)
  3. Testes de Contrato de Frameworks (38 testes: Speech, AVFoundation, CoreML, SwiftData)
  4. Testes de Engenharia de Caos (16 cenÃ¡rios de resiliÃªncia)
  5. Benchmarks de Performance + DetecÃ§Ã£o de RegressÃ£o
  6. GeraÃ§Ã£o de RelatÃ³rios HTML + Deploy no GitHub Pages

**DetecÃ§Ã£o AutomÃ¡tica de RegressÃ£o**:
- Build falha se o Resilience Score cair >10%
- Rastreamento histÃ³rico em banco SQLite (`test_artifacts/performance_trends.db`)
- Alertas para cenÃ¡rios com falhas novas ou scores crÃ­ticos (<50)

**Dashboard Interativo**:
- GrÃ¡ficos com Chart.js: score trend, scores por categoria, pass rates de cenÃ¡rios
- Deploy automÃ¡tico no GitHub Pages: `https://drleandroalm.github.io/Swift_Audio/`
- Atualizado a cada push para `main`

**Scripts**:
```bash
# Registrar resultados no banco de dados
./Scripts/record_test_run.sh --scorecard test_artifacts/ResilienceScorecard.json

# Detectar regressÃµes (exit code 1 se houver issues crÃ­ticos)
./Scripts/detect_regressions.swift check

# Gerar dashboard HTML
./Scripts/generate_html_report.swift --output test_artifacts/html_report

# Visualizar localmente
open test_artifacts/html_report/index.html
```

**DocumentaÃ§Ã£o Completa**: Ver `test_artifacts/PHASE3_CI_CD_PIPELINE_SUMMARY.md`

## ğŸ”§ Build & Test RÃ¡pidos

- macOS (ARM64) build: `xcodebuild -scheme SwiftScribe -destination 'platform=macOS,arch=arm64' build`
- macOS tests: `xcodebuild -scheme SwiftScribe -destination 'platform=macOS,arch=arm64' test`
- iOS Simulator build: `xcodebuild -scheme SwiftScribe -destination 'platform=iOS Simulator,name=iPhone 16 Pro' build`
 - iOS Simulator tests: `xcodebuild -scheme SwiftScribe-iOS-Tests -destination 'platform=iOS Simulator,name=iPhone 16 Pro' test`

## ğŸ“¦ Modelos Empacotados (Offline-First)

- Os modelos Core ML de diarizaÃ§Ã£o sÃ£o empacotados no aplicativo via `speaker-diarization-coreml/` (referÃªncia de pasta na fase de recursos). O app inicia totalmente offline.
- Ordem de resoluÃ§Ã£o: `FLUID_AUDIO_MODELS_PATH` â†’ bundle do app `speaker-diarization-coreml/` â†’ pasta do repositÃ³rio. Downloads remotos estÃ£o desativados por padrÃ£o; ausÃªncia local gera erro de configuraÃ§Ã£o claro.

## ğŸ§ª VerificaÃ§Ã£o de CI dos Modelos

- O script `Scripts/verify_bundled_models.sh` compila macOS e iOS (simulador) e verifica a presenÃ§a de `pyannote_segmentation.mlmodelc` e `wespeaker_v2.mlmodelc` (incluindo `coremldata.bin`) dentro dos bundles gerados.
- O workflow do GitHub Actions `.github/workflows/ci.yml` executa automaticamente essa verificaÃ§Ã£o em pushes/PRs.
  - Inclui um alvo de testes especÃ­fico do iOS (`ScribeTests_iOS`) para validar a presenÃ§a dos modelos no bundle do simulador.
  - Opcional: vocÃª pode adicionar um passo para rodar a CLI de smoke (`Scripts/RecorderSmokeCLI/run_cli.sh`) sobre um WAV de referÃªncia para cobertura determinÃ­stica do pipeline de transcriÃ§Ã£o, evitando a flutuaÃ§Ã£o de finalize no XCTest do macOS.

### Registro do Esquema de URL (swiftscribe)

Para habilitar `swiftscribe://record` fora de builds de Debug (e permitir que o sistema abra o app via URL), registre o esquema no Info do alvo:

1) No Xcode, selecione o alvo do app â†’ aba â€œInfoâ€.
2) Em â€œURL Typesâ€, clique em â€œ+â€ e preencha:
   - Identifier: `com.swift.examples.scribe.urlscheme` (qualquer string Ãºnica)
   - URL Schemes: `swiftscribe`
   - Role: `Editor`
3) Compile e rode. Agora `swiftscribe://record` aciona o handler no app.

Notas:
- A alternÃ¢ncia â€œPermitir acionar gravaÃ§Ã£o via URLâ€ em ConfiguraÃ§Ãµes controla o comportamento no runtime (pode desabilitar a automaÃ§Ã£o).
- Em macOS, o handler posta uma notificaÃ§Ã£o interna que navega para o memo e inicia a gravaÃ§Ã£o.

#### Snippet Info.plist (alternativa manual)

Se preferir editar o Info.plist manualmente, adicione uma entrada `CFBundleURLTypes` conforme abaixo:

```xml
<key>CFBundleURLTypes</key>
<array>
  <dict>
    <key>CFBundleURLName</key>
    <string>com.swift.examples.scribe.urlscheme</string>
    <key>CFBundleURLSchemes</key>
    <array>
      <string>swiftscribe</string>
    </array>
    <key>CFBundleTypeRole</key>
    <string>Editor</string>
  </dict>
  <!-- opcional: adicionar mais esquemas no futuro -->
  </array>
```

ApÃ³s salvar, faÃ§a um clean build e execute o app. O sistema passa a reconhecer `swiftscribe://record` como uma URL vÃ¡lida para abrir o app.

## ğŸ’¾ ExportaÃ§Ã£o de Falantes (macOS)

- ExportaÃ§Ãµes de transcriÃ§Ã£o/unificada usam `NSSavePanel` (com direito de Leitura/GravaÃ§Ã£o habilitado). A exportaÃ§Ã£o de falantes tambÃ©m usa `NSSavePanel` e, caso falhe/indisponÃ­vel, cai para a pasta Documentos do container com revelaÃ§Ã£o no Finder.

## ğŸ—º Development Roadmap & Future Features

> Confira o estado atual e os prÃ³ximos marcos em [`ROADMAP.md`](ROADMAP.md).

### **Phase 1: Core Features** âœ… **COMPLETED**

- âœ… Real-time speech transcription
- âœ… On-device AI processing  
- âœ… Rich text editing
- âœ… **Professional speaker diarization** with FluidAudio integration
- âœ… **Speaker attribution** and visual formatting
- âœ… Fully offline startup via bundled Core ML models
- âœ… Live diarization updates with adjustable processing window

### **Phase 2: Advanced Features** 

- ğŸ”Š **Output audio tap** for system audio capture
- ğŸŒ **Enhanced multi-language** support
- ğŸ“Š **Advanced analytics** and speaker insights
- ğŸ¯ **Speaker voice profiles** and personalization

## ğŸ“„ License & Legal

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for complete details.

## ğŸ™ Acknowledgments & Credits

- **Apple WWDC 2025** sessions on SpeechAnalyzer, Foundation Models, and Rich Text editing
- **Apple Developer Frameworks** - SpeechAnalyzer, Foundation Models, Rich Text Editor
- **FluidAudio** - Professional speaker diarization and voice identification technology

## ğŸš€ Getting Started with AI Development Tools

**For Cursor & Windsurf IDE users:** Leverage AI agents to explore the comprehensive documentation in the `Docs/` directory, featuring complete WWDC 2025 session transcripts covering:

- ğŸ¤ **SpeechAnalyzer & SpeechTranscriber** API implementation guides
- ğŸ¤– **Foundation Models** framework integration
- âœï¸ **Rich Text Editor** advanced capabilities  
- ğŸ”Š **Audio processing** improvements and optimizations

---

**â­ Star this repo** if you find it useful! | **ğŸ”— Share** with developers interested in AI-powered speech transcription
### iOS Import/Export (Files)
- Na visÃ£o â€œFalantesâ€ no iOS, use os botÃµes de Exportar/Importar (Ã­cones de upload/download) para salvar/carregar perfis de falantes (.json) via app Arquivos.
- O export gera um JSON com `id`, `nome`, `cor` e `embedding` por falante. A importaÃ§Ã£o adiciona/atualiza perfis e injeta no runtime do diarizador.

### iOS Exportar TranscriÃ§Ã£o (Files)
- Ainda na visÃ£o â€œFalantesâ€, toque em â€œExportar transcriÃ§Ã£oâ€ e escolha entre JSON (segmentos com `speakerName`, inÃ­cio/fim, confianÃ§a, texto) ou Markdown (transcriÃ§Ã£o etiquetada por falante).
